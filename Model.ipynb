{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "trainset = datasets.MNIST('TrainSet', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('ValSet', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, Y = dataiter.next()\n",
    "images = images.numpy()\n",
    "X = np.resize(images,(64,784))\n",
    "Y = Y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newY = np.zeros(shape=(64,10))\n",
    "for i in range(Y.shape[0]):\n",
    "    newY[i,Y[i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSigmoid(T, x):\n",
    "    numerator = np.exp(np.matmul(x,T))\n",
    "    denomenator = numerator + 1 \n",
    "    sig = np.divide(numerator,denomenator)\n",
    "    return sig\n",
    "\n",
    "def getLoss(T, Y, x):\n",
    "    m = np.prod(Y.shape)\n",
    "    activation = getSigmoid(T,x)\n",
    "    loss= np.square(np.subtract(activation, Y))\n",
    "    print(loss)\n",
    "    loss = np.sum(loss)\n",
    "    print(loss)\n",
    "    loss = np.multiply(loss, 1/(2 * m))\n",
    "    return loss\n",
    "\n",
    "def getGradient(weights, Y, X):\n",
    "    activation = getSigmoid(weights,X)\n",
    "    z = np.square(np.subtract(activation, Y))\n",
    "    gradient = np.matmul(X.transpose(), z)\n",
    "    print(z)\n",
    "    return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.84139381e-235 5.35008298e-223 1.66960994e-230 4.12226733e-221\n",
      "  2.94813532e-229 6.48606254e-224 1.64540403e-233 2.97100982e-229\n",
      "  1.78374349e-215 1.00000000e+000]\n",
      " [5.01607210e-237 1.13023212e-242 6.14313229e-241 5.30410242e-237\n",
      "  1.97758608e-251 1.11459873e-239 2.46495320e-247 8.38190510e-245\n",
      "  1.00000000e+000 9.53030652e-237]\n",
      " [1.24062296e-212 4.25507688e-217 1.00000000e+000 1.69027843e-220\n",
      "  1.03021055e-225 1.03753788e-215 6.78772286e-212 1.49653265e-225\n",
      "  6.15536875e-210 4.40026991e-214]\n",
      " [9.05706369e-260 1.75849859e-257 6.81507583e-260 2.61263553e-263\n",
      "  4.26610061e-270 1.00000000e+000 5.35660680e-266 6.30700168e-260\n",
      "  1.80628445e-247 8.35203445e-254]\n",
      " [2.93708461e-274 3.12950387e-272 5.56303091e-276 2.07745522e-271\n",
      "  7.89864887e-277 1.51302006e-269 8.27759281e-280 1.70236833e-279\n",
      "  1.52737090e-259 1.00000000e+000]\n",
      " [1.00000000e+000 1.13092082e-198 2.53206659e-194 6.14566051e-187\n",
      "  3.50572502e-198 6.44402871e-195 2.94850781e-185 5.44725540e-201\n",
      "  5.23292272e-194 8.07420692e-189]\n",
      " [1.82027362e-279 1.43850810e-275 1.37662464e-285 1.67740550e-273\n",
      "  4.00553051e-289 9.79704435e-276 1.00000000e+000 1.58699702e-282\n",
      "  1.37187708e-269 2.84191990e-282]\n",
      " [1.00000000e+000 7.69273512e-231 4.12604852e-228 1.96014401e-228\n",
      "  2.08916697e-236 6.40717218e-231 2.54020182e-224 8.52459764e-235\n",
      "  7.27591754e-219 1.69398808e-232]\n",
      " [5.67614634e-213 2.09959102e-213 1.32129563e-222 2.42493115e-208\n",
      "  1.23752223e-222 1.00000000e+000 8.15305124e-219 1.05582130e-230\n",
      "  4.75138400e-209 2.68336440e-215]\n",
      " [2.77172281e-253 7.73051465e-249 2.03516858e-249 3.37368652e-242\n",
      "  1.00000000e+000 1.25663154e-240 2.96827841e-252 3.15733629e-249\n",
      "  1.02258319e-237 3.04015123e-241]\n",
      " [2.91971236e-236 1.00000000e+000 1.36273873e-246 4.34208713e-246\n",
      "  2.22537594e-257 6.74592754e-245 1.21016870e-249 8.85529284e-252\n",
      "  4.39293821e-242 4.31291992e-246]\n",
      " [3.08288868e-278 1.36953043e-278 1.10753944e-288 1.00000000e+000\n",
      "  4.23470094e-288 1.05652319e-275 6.96870001e-282 1.60570707e-289\n",
      "  3.78489011e-273 5.83752956e-283]\n",
      " [3.88679323e-235 1.55566473e-238 1.00000000e+000 2.37132527e-234\n",
      "  2.78087840e-242 9.26639362e-242 1.30312936e-241 3.92404148e-244\n",
      "  2.65793128e-231 3.22095541e-234]\n",
      " [1.20477438e-260 4.92009956e-252 6.99728495e-264 5.95296194e-255\n",
      "  1.00000000e+000 9.53979658e-252 1.22178350e-265 1.91942980e-263\n",
      "  2.33982025e-247 1.08387816e-247]\n",
      " [2.38610816e-245 4.21564228e-240 1.00000000e+000 6.62069017e-245\n",
      "  1.61073347e-245 3.43055913e-237 1.91760395e-249 8.35559285e-249\n",
      "  4.55791590e-239 3.29915137e-236]\n",
      " [7.08943763e-235 1.85333338e-231 4.95259119e-236 5.23425476e-227\n",
      "  6.99883838e-247 1.97242959e-226 1.32432450e-234 3.09876847e-242\n",
      "  2.98686730e-222 1.00000000e+000]\n",
      " [7.27043238e-241 2.93811556e-246 1.00000000e+000 4.20664914e-245\n",
      "  1.14387098e-265 2.14453465e-243 5.68764875e-249 2.60262071e-250\n",
      "  1.61876809e-238 5.91999246e-250]\n",
      " [9.86506411e-248 2.17699854e-253 2.77533487e-244 8.64026309e-247\n",
      "  1.00000000e+000 1.35513573e-238 9.15918697e-253 8.62954981e-251\n",
      "  1.37808187e-236 1.18057145e-246]\n",
      " [2.96561214e-262 7.12537738e-256 8.58177259e-270 1.99626401e-254\n",
      "  2.99815180e-264 1.67175604e-256 2.45760052e-262 1.00000000e+000\n",
      "  9.21483985e-248 1.69137461e-259]\n",
      " [5.91577233e-291 9.78895970e-290 6.06038294e-301 1.71370696e-288\n",
      "  1.00000000e+000 1.09150476e-291 2.07956260e-294 6.25397956e-299\n",
      "  1.99708567e-284 4.54107400e-295]\n",
      " [1.14890866e-232 1.82636769e-233 2.81947776e-241 4.73574745e-235\n",
      "  5.18143916e-246 3.85967894e-235 1.38048689e-237 9.15154008e-243\n",
      "  1.00000000e+000 1.94375966e-237]\n",
      " [2.41727634e-260 2.51168949e-270 3.14953784e-267 1.20940927e-264\n",
      "  1.12123441e-279 1.42315854e-256 1.00000000e+000 1.44731605e-274\n",
      "  1.55465285e-261 2.02784192e-269]\n",
      " [6.12042674e-262 9.58785782e-268 1.00014678e-270 4.84565723e-267\n",
      "  5.15299276e-278 2.34372953e-267 2.61953222e-267 3.99467908e-274\n",
      "  1.00000000e+000 2.03394207e-265]\n",
      " [1.24276491e-235 3.07660695e-249 1.00000000e+000 4.65037499e-249\n",
      "  1.78824450e-260 1.24534720e-251 1.53630280e-247 1.69416309e-257\n",
      "  2.03987922e-241 2.79994145e-250]\n",
      " [2.25875114e-273 4.48206632e-262 2.48746092e-268 1.68178281e-263\n",
      "  8.62938039e-279 1.00000000e+000 3.83267753e-274 1.28715477e-267\n",
      "  6.66816908e-253 3.95202051e-268]\n",
      " [6.77464062e-236 2.06029906e-238 1.00000000e+000 7.28330597e-246\n",
      "  2.09289011e-255 6.02426147e-239 5.29817604e-247 1.56478287e-250\n",
      "  1.63209335e-238 9.11112992e-250]\n",
      " [1.21908389e-171 1.29259579e-181 1.00000000e+000 2.06364305e-181\n",
      "  2.69364463e-183 4.73406919e-176 1.88631506e-177 5.69281852e-181\n",
      "  5.86002396e-179 2.10693684e-177]\n",
      " [6.17841047e-288 7.63775915e-287 5.30194015e-290 4.28537434e-281\n",
      "  1.00000000e+000 2.16410356e-286 2.95620498e-294 5.69769332e-289\n",
      "  1.10498603e-280 5.31557521e-290]\n",
      " [1.00000000e+000 9.85281283e-206 2.13565309e-197 6.55476710e-196\n",
      "  6.21865210e-203 2.86511351e-201 3.71262026e-188 1.56600136e-199\n",
      "  7.71831067e-199 1.83269527e-193]\n",
      " [1.00000000e+000 6.81604604e-263 3.31174571e-266 2.12847028e-264\n",
      "  1.07029846e-273 6.39344113e-262 1.28567876e-264 2.53737717e-274\n",
      "  7.40981645e-259 2.19033401e-260]\n",
      " [5.15752323e-249 5.83515089e-248 3.44556762e-251 6.34986109e-248\n",
      "  4.78503956e-255 7.54105183e-250 2.40316859e-256 5.06908018e-253\n",
      "  5.96655628e-243 1.00000000e+000]\n",
      " [4.94862807e-248 5.94326867e-255 2.29956437e-261 1.50772125e-256\n",
      "  3.62622529e-262 2.06957334e-254 1.21858535e-259 2.18635586e-261\n",
      "  1.00000000e+000 1.72516307e-253]\n",
      " [1.04831647e-226 1.49654240e-232 1.25890408e-238 1.88530487e-235\n",
      "  1.00000000e+000 1.66386488e-224 1.77703870e-234 2.72508632e-244\n",
      "  6.83597154e-234 9.74708509e-232]\n",
      " [1.67954252e-302 1.00000000e+000 2.74761861e-313 2.74897218e-304\n",
      "  2.37151510e-321 2.61243941e-304 1.98255114e-313 4.37530406e-317\n",
      "  7.81048282e-298 6.87396311e-311]\n",
      " [2.81585189e-264 1.84643712e-269 2.42189753e-271 1.69788311e-260\n",
      "  4.88990378e-279 1.00000000e+000 3.26389989e-272 2.98710121e-271\n",
      "  2.77022441e-258 1.36649636e-270]\n",
      " [4.35679374e-180 8.89813997e-177 1.45130097e-182 6.31492812e-173\n",
      "  1.26510501e-188 9.13824763e-178 4.52745952e-176 7.70637106e-174\n",
      "  1.00000000e+000 2.99921039e-172]\n",
      " [2.51106094e-212 2.49900580e-212 3.80326783e-216 2.59646828e-213\n",
      "  2.45962183e-230 1.00000000e+000 7.09359552e-215 5.38711757e-221\n",
      "  2.94428103e-205 5.24361348e-209]\n",
      " [1.94257338e-267 6.41322052e-265 5.50029364e-272 4.44243594e-270\n",
      "  5.17719825e-276 1.81985085e-264 6.65587544e-268 2.73723540e-276\n",
      "  1.15841385e-258 1.00000000e+000]\n",
      " [1.00000000e+000 3.10804180e-222 4.18338336e-227 1.40265808e-215\n",
      "  1.48281329e-225 9.72741488e-216 1.39743919e-212 3.56979966e-228\n",
      "  7.42404392e-217 3.41447727e-221]\n",
      " [8.24651253e-230 6.02694233e-232 6.10450658e-233 1.61410156e-229\n",
      "  1.73908943e-241 4.72135383e-235 3.34301392e-234 3.47775205e-242\n",
      "  1.00000000e+000 2.22723656e-232]\n",
      " [2.70453185e-298 1.00000000e+000 2.75869814e-303 1.70680797e-296\n",
      "  4.26350041e-312 3.73901675e-295 2.62079083e-302 9.35319383e-301\n",
      "  2.31216029e-291 1.95745440e-302]\n",
      " [1.24437936e-291 4.40470104e-293 1.48617605e-292 1.71913143e-288\n",
      "  1.00000000e+000 2.25904870e-285 1.25512753e-293 3.64117577e-296\n",
      "  5.20303726e-283 1.05798748e-293]\n",
      " [2.04298437e-213 2.07123972e-224 8.24060342e-223 4.33063076e-220\n",
      "  8.29711121e-239 7.06104863e-221 1.00000000e+000 1.43793620e-230\n",
      "  3.01927120e-220 8.46437262e-224]\n",
      " [8.75621926e-280 2.67991845e-282 2.54722861e-289 7.42121322e-277\n",
      "  7.27292306e-291 5.75504227e-276 1.00000000e+000 7.43553273e-290\n",
      "  1.25306063e-268 1.78279924e-284]\n",
      " [2.20949485e-283 7.89774425e-275 1.38028427e-288 2.98904656e-277\n",
      "  8.08829393e-285 1.67198479e-273 8.42796650e-280 1.00000000e+000\n",
      "  4.30531327e-268 5.76202969e-284]\n",
      " [4.78814660e-272 9.22351584e-269 2.56336803e-276 6.66956147e-260\n",
      "  1.87536822e-283 1.00000000e+000 1.68924418e-273 2.40678216e-281\n",
      "  3.93742098e-271 2.55919303e-273]\n",
      " [3.02265037e-230 4.84917670e-235 1.79839913e-240 1.00000000e+000\n",
      "  1.61717391e-248 2.23163350e-234 2.60283299e-242 1.94395353e-246\n",
      "  5.84156454e-227 1.50733965e-236]\n",
      " [5.91095930e-282 1.08105728e-279 9.51713737e-288 2.22997129e-282\n",
      "  1.39246816e-290 1.43817912e-283 1.55580442e-293 5.03413261e-287\n",
      "  8.63444916e-276 1.00000000e+000]\n",
      " [4.60229714e-232 7.89226669e-233 1.00000000e+000 6.82921652e-236\n",
      "  1.65108287e-240 1.52143499e-238 1.61201870e-237 1.02243014e-246\n",
      "  1.00368888e-231 6.36349905e-235]\n",
      " [2.27960559e-215 1.66730989e-218 3.80186046e-223 1.00000000e+000\n",
      "  1.31350211e-234 6.06732068e-218 2.64156182e-222 6.08673945e-229\n",
      "  9.11220834e-215 3.13196986e-216]\n",
      " [1.34378438e-216 6.48417085e-214 1.00000000e+000 3.53051661e-213\n",
      "  1.20682910e-219 2.50363571e-220 1.18302583e-212 1.30994226e-226\n",
      "  1.09956582e-209 1.69985481e-206]\n",
      " [3.67044931e-239 5.19091993e-240 1.68811266e-243 6.04471400e-241\n",
      "  2.79668128e-245 1.09071887e-231 9.46965849e-242 6.32005735e-242\n",
      "  1.77489082e-230 1.00000000e+000]\n",
      " [2.10091528e-216 1.25285400e-232 1.11195431e-227 1.00000000e+000\n",
      "  2.34678141e-239 6.71589733e-228 2.88799991e-232 5.92704648e-236\n",
      "  8.84719673e-225 4.67097230e-229]\n",
      " [2.05769796e-239 1.13671802e-235 9.35327694e-245 3.83809839e-229\n",
      "  4.25894355e-245 6.36050222e-235 1.00000000e+000 1.50130452e-241\n",
      "  4.75593751e-231 3.75731346e-238]\n",
      " [3.19538090e-228 1.68434998e-224 3.62236086e-230 1.99881296e-220\n",
      "  2.19056853e-238 3.64015027e-225 1.00000000e+000 2.87512702e-231\n",
      "  3.37693240e-222 4.49560791e-228]\n",
      " [1.00000000e+000 5.91353910e-171 2.96970893e-178 9.61195727e-173\n",
      "  2.70159306e-170 3.98572836e-181 1.49711193e-168 4.31552685e-180\n",
      "  2.67058553e-172 1.13314183e-164]\n",
      " [1.00000000e+000 7.60950115e-268 9.34124069e-278 3.50201193e-267\n",
      "  4.65146802e-284 1.67040312e-271 1.66757824e-259 3.02795784e-273\n",
      "  8.85844200e-262 4.86244736e-267]\n",
      " [1.82233639e-303 1.00000000e+000 7.74042766e-319 1.29889602e-307\n",
      "  0.00000000e+000 7.94463233e-308 6.54595479e-316 7.85860816e-319\n",
      "  1.36725553e-299 2.77046679e-316]\n",
      " [3.51428249e-282 1.00000000e+000 3.37800722e-298 5.78911194e-290\n",
      "  1.94296783e-307 9.61038077e-291 9.86173459e-301 1.15539070e-301\n",
      "  2.00910296e-281 6.90961111e-298]\n",
      " [2.53815366e-268 1.45463999e-262 2.89604704e-271 1.15717931e-264\n",
      "  4.37316827e-276 2.78081229e-263 1.69909189e-274 1.00000000e+000\n",
      "  3.49611845e-261 9.15551417e-267]\n",
      " [2.70047900e-267 1.32971582e-266 2.00101366e-269 4.09259340e-266\n",
      "  5.77374989e-273 6.64710640e-263 3.40221276e-264 1.00000000e+000\n",
      "  1.53971742e-257 2.32958680e-269]\n",
      " [1.19500393e-238 3.12723633e-250 8.20366438e-254 1.00000000e+000\n",
      "  2.63662083e-259 9.22169154e-252 1.94222382e-246 4.57485484e-255\n",
      "  2.10007129e-238 4.77122505e-256]\n",
      " [5.07869182e-280 3.28080395e-280 4.65179849e-285 6.10332916e-285\n",
      "  4.13460114e-300 7.48809094e-282 8.68295692e-288 9.66106964e-291\n",
      "  3.24708882e-273 1.00000000e+000]\n",
      " [1.00000000e+000 8.79829168e-204 2.43195694e-203 6.51683733e-198\n",
      "  1.50957913e-198 1.04989160e-204 5.66933719e-195 5.59208959e-208\n",
      "  1.09718777e-198 1.33421457e-192]]\n",
      "64.0\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.rand(784,10)\n",
    "grad = getLoss(theta, newY, X)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yotam\\AppData\\Local\\Temp\\ipykernel_24568\\224277179.py:22: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(-(y * np.log(predictions) + (1 - y) * np.log(1 - predictions)))\n",
      "C:\\Users\\yotam\\AppData\\Local\\Temp\\ipykernel_24568\\224277179.py:22: RuntimeWarning: invalid value encountered in multiply\n",
      "  return np.mean(-(y * np.log(predictions) + (1 - y) * np.log(1 - predictions)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 5.00166940e-113, 1.00000000e+000, 5.59477905e-111,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       3.58661229e-106, 1.97144794e-113, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.92839245e-107, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 5.44519536e-112,\n",
       "       2.00885039e-114, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 8.63596832e-114])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_iterations = 100\n",
    "learning_rate = .01\n",
    "\n",
    "# Initialize the model parameters\n",
    "# Weights are represented as a matrix with one row for each sample and one column for each pixel\n",
    "# Bias is a scalar value\n",
    "weights = np.random.rand(X.shape[0], X.shape[1])\n",
    "bias = np.random.rand()\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Implement the logistic regression model\n",
    "def predict(x, weights, bias):\n",
    "  return sigmoid(np.dot(x, weights.T) + bias)\n",
    "\n",
    "# Calculate the loss of the model on the training set\n",
    "def loss(predictions, y):\n",
    "  return np.mean(-(y * np.log(predictions) + (1 - y) * np.log(1 - predictions)))\n",
    "\n",
    "# Train the model using gradient descent\n",
    "for i in range(num_iterations):\n",
    "  # Make predictions on the training set\n",
    "  predictions = predict(X, weights, bias)\n",
    "\n",
    "  # Calculate the loss of the model on the training set\n",
    "  l = loss(predictions, Y)\n",
    "\n",
    "  # Calculate the gradient of the loss with respect to the model parameters\n",
    "  gradient_weights = np.dot(X.T, (predictions - Y)) / X.shape[0]\n",
    "  gradient_bias = np.mean(predictions) - Y\n",
    "\n",
    "  # Update the model parameters using the gradient and a learning rate\n",
    "  weights -= learning_rate * gradient_weights.transpose()\n",
    "  bias -= learning_rate * gradient_bias\n",
    "\n",
    "predict(X[0], weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of classes\n",
    "K = 10\n",
    "\n",
    "# Number of features\n",
    "N = 784\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros((K,N))\n",
    "\n",
    "#Training Iterations\n",
    "trainIters = 100\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training method\n",
    "def one_vs_all_logreg(X, y, K, weights, learning_rate):\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(trainIters):\n",
    "    # Initialize the cost\n",
    "      J = 0\n",
    "      \n",
    "      # Compute the hypothesis\n",
    "      h = np.dot(X, weights.T)\n",
    "      \n",
    "      # Compute the cost\n",
    "      for k in range(K):\n",
    "          temp = -y[:,k] * np.log(sigmoid(h[:,k])) - (1 - y[:,k]) * np.log(1 - sigmoid(h[:,k]))\n",
    "          J += np.sum(temp)\n",
    "          g = np.dot(X.T, (sigmoid(h)[:,k] - y[:,k]))\n",
    "          g = g/m\n",
    "          \n",
    "\n",
    "          #update weights \n",
    "          weights[k] = weights[k] - learning_rate*g\n",
    "\n",
    "          return weights\n",
    "          \n",
    "      J = J/m\n",
    "      #J += np.sum(weights**2)\n",
    "      print(J)\n",
    "      \n",
    "    \n",
    "\n",
    "\n",
    "theta = one_vs_all_logreg(X, newY, K, weights, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def predict(X,Weights):\n",
    "    return argmax(sigmoid(np.dot(X, weights.T)))\n",
    "\n",
    "print(predict(X[0,:], theta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41a8bb042236853c4d003d0bbfa761c6f3a71c5f238884c2a97247d6653e3ced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
